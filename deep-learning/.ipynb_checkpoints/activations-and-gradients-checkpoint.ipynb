{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70dd86f-f70c-43fe-9a17-4870bb6d431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bd4ec6-69b9-4101-97f7-3c04a9ce5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt','r').read().splitlines()\n",
    "chars = sorted(list(set((''.join(names)))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {stoi[s]: s for s in stoi}\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78544f3b-0e3b-4266-8e3b-cece472457f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_test_data(names, block_size=3, n=None):\n",
    "    X, y = [], []\n",
    "    for name in names[:n]:\n",
    "        context = [0] * block_size\n",
    "        for char in name + '.':\n",
    "            i = stoi[char]\n",
    "            X.append(context)\n",
    "            y.append(i)\n",
    "            if n is not None and n < 10:\n",
    "                print(''.join(itos[j] for j in context), '->', itos[i])\n",
    "            context = context[1:] + [i]\n",
    "    \n",
    "    zipped = list(zip(X, y))\n",
    "    random.shuffle(zipped)\n",
    "    X, y = zip(*zipped)\n",
    "    \n",
    "    n_dev = int(0.8 * len(X))\n",
    "    n_test = int(0.9 * len(X))\n",
    "    \n",
    "    X_train = torch.tensor(X[:n_dev])\n",
    "    y_train = torch.tensor(y[:n_dev])\n",
    "    \n",
    "    X_dev = torch.tensor(X[n_dev:n_test])\n",
    "    y_dev = torch.tensor(y[n_dev:n_test])\n",
    "    \n",
    "    X_test = torch.tensor(X[n_test:])\n",
    "    y_test = torch.tensor(y[n_test:])\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e58d5e62-573a-4606-8428-97532669a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embed = 4\n",
    "block_size = 6\n",
    "n_hid = 300\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = train_dev_test_data(names, block_size=block_size)\n",
    "C = torch.randn(vocab_size, dim_embed, requires_grad=True)\n",
    "W1 = torch.randn(dim_embed * block_size, n_hid, requires_grad=True)\n",
    "b1 = torch.randn(n_hid, requires_grad=True)\n",
    "W2 = torch.randn(n_hid, vocab_size, requires_grad=True) \n",
    "b2 = torch.randn(vocab_size, requires_grad=True) * 0\n",
    "params = [C, W1, b1, W2, b2]\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e279ef5a-4eac-4b23-b6f1-48384e214540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cip19aac\\AppData\\Local\\Temp\\ipykernel_20260\\1808697369.py:22: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  p.data -= a * p.grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# gradient descent\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m---> 22\u001b[0m         p\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\n\u001b[0;32m     24\u001b[0m     lossi\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(lossi)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "a = 0.1 # from plotting loss against different a and taking minimum\n",
    "sz = 100\n",
    "epochs = 5000\n",
    "\n",
    "for e in range(epochs):\n",
    "    # minibatch\n",
    "    rows = torch.randint(0, X_train.shape[0], (sz, ))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X_train[rows]]\n",
    "    h = torch.tanh(emb.view(-1, dim_embed * block_size) @ W1 + b1) # 6 = block_size * dim(embedding space), basically want to get the embedding per input all on one row\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y_train[rows])\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient descent\n",
    "    for p in params:\n",
    "        p.data -= a * p.grad\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    \n",
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6bc2d8f-84b8-4e5a-bbcc-f42cdfa1fb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.3262288570404053\n",
      "Dev loss: 3.3248205184936523\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def compute_loss(split):\n",
    "    X, y = {\n",
    "        'train': (X_train, y_train),\n",
    "        'dev': (X_dev, y_dev),\n",
    "        'test': (X_test, y_dev)\n",
    "    }[split]\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1, dim_embed * block_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss\n",
    "\n",
    "print(f'Training loss: {compute_loss('train')}')\n",
    "print(f'Dev loss: {compute_loss('dev')}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a26bfeba-e83f-4696-88a7-690212994802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "canlanno.\n",
      "lyeinn.\n",
      "aliannna.\n",
      "jhyla.\n",
      "cyri.\n",
      "arinen.\n",
      "kyevee.\n",
      "reiva.\n",
      "keleel.\n"
     ]
    }
   ],
   "source": [
    "def make_name(k):\n",
    "    g = torch.Generator().manual_seed(2147483647 + k)\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        i = torch.multinomial(p, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [i]\n",
    "        out.append(i)\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "    return ''.join([itos[i] for i in out])\n",
    "\n",
    "for k in range(10):\n",
    "    print(make_name(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86bc6b-8248-4c6f-9042-c013e6a79339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
