{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70dd86f-f70c-43fe-9a17-4870bb6d431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30bd4ec6-69b9-4101-97f7-3c04a9ce5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt','r').read().splitlines()\n",
    "chars = sorted(list(set((''.join(names)))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {stoi[s]: s for s in stoi}\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78544f3b-0e3b-4266-8e3b-cece472457f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_test_data(names, context_len=3, n=None):\n",
    "    X, y = [], []\n",
    "    for name in names[:n]:\n",
    "        context = [0] * context_len\n",
    "        for char in name + '.':\n",
    "            i = stoi[char]\n",
    "            X.append(context)\n",
    "            y.append(i)\n",
    "            if n is not None and n < 10:\n",
    "                print(''.join(itos[j] for j in context), '->', itos[i])\n",
    "            context = context[1:] + [i]\n",
    "    \n",
    "    zipped = list(zip(X, y))\n",
    "    random.shuffle(zipped)\n",
    "    X, y = zip(*zipped)\n",
    "    \n",
    "    n_dev = int(0.8 * len(X))\n",
    "    n_test = int(0.9 * len(X))\n",
    "    \n",
    "    X_train = torch.tensor(X[:n_dev])\n",
    "    y_train = torch.tensor(y[:n_dev])\n",
    "    \n",
    "    X_dev = torch.tensor(X[n_dev:n_test])\n",
    "    y_dev = torch.tensor(y[n_dev:n_test])\n",
    "    \n",
    "    X_test = torch.tensor(X[n_test:])\n",
    "    y_test = torch.tensor(y[n_test:])\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb09117-523e-4cb6-92a3-8be13a4b4d0d",
   "metadata": {},
   "source": [
    "# Making the code like PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78a415ca-d600-4802-bda4-55e7d1a296db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in ** 0.5 # <- gain\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([self.bias] if self.bias is not None else [])\n",
    "\n",
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, fan_in, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(1, fan_in)\n",
    "        self.beta = torch.zeros(1, fan_in)\n",
    "        self.running_mean = torch.zeros(1, fan_in)\n",
    "        self.running_var = torch.ones(1, fan_in)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.dim == 3:\n",
    "                dim = (0,1)\n",
    "            xmean = x.mean(dim, keepdim=True)\n",
    "            xvar = x.var(dim, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var  = (1 - self.momentum) * self.running_var  + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Embedding:\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "\n",
    "    def __call__(self, ix):\n",
    "        self.out = self.weight[ix]\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class FlattenConsecutive:\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        n = self.n\n",
    "        x = x.view(B, T//n, C*n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def training_mode(self):\n",
    "        for layer in self.layers:\n",
    "            layer.training = True\n",
    "\n",
    "    def inference_mode(self):\n",
    "        for layer in self.layers:\n",
    "            layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "105b5ed1-27bd-48be-bc88-aa36cc20707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22397\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "dim_embed = 10\n",
    "context_len = 8\n",
    "n_hid = 68\n",
    "softmax_scale = 0.01 # ensure softmax gives uniform distribution output\n",
    "kaiming_init = (5 / 3) / ((dim_embed * context_len) ** 0.5) # gain to modify activation std and reduce tanh saturation\n",
    "M = 0.001\n",
    "\n",
    "# data and parameters\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = train_dev_test_data(names, context_len=context_len)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, dim_embed),\n",
    "    FlattenConsecutive(2), Linear(dim_embed * 2, n_hid, bias=False), BatchNorm1d(n_hid), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(    n_hid * 2, n_hid, bias=False), BatchNorm1d(n_hid), Tanh(),\n",
    "    FlattenConsecutive(2), Linear(    n_hid * 2, n_hid, bias=False), BatchNorm1d(n_hid), Tanh(),\n",
    "    Linear(n_hid, vocab_size),\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1 # # ensure last layer -- logit layer -- feeding forward into softmax initially gives uniform distribution output to reduce loss\n",
    "    for layer in model.layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3\n",
    "\n",
    "params = model.parameters()\n",
    "print(sum(p.nelement() for p in params))\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e8f4dda-be81-4eac-90c3-79925083bef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2111\n"
     ]
    }
   ],
   "source": [
    "epochs = 200000\n",
    "minibatch_sz = 32\n",
    "lossi = []\n",
    "updates = []\n",
    "a = 0.1\n",
    "\n",
    "for e in range(epochs):\n",
    "    # minibatches\n",
    "    rows = torch.randint(0, X_train.shape[0], (minibatch_sz, ))\n",
    "    Xb, yb = X_train[rows], y_train[rows]\n",
    "\n",
    "    # forward pass\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # stochastic gradient descent\n",
    "    a = a if e < epochs / 2 else 0.1 * a\n",
    "    for p in params:\n",
    "        p.data -= a * p.grad\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    \n",
    "    if e % 10000 == 0:\n",
    "        print(f'{e:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "\n",
    "    if e > 0:\n",
    "        break\n",
    "\n",
    "#plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0aebb34-7025-41a1-996f-86a328f059ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.0202\n",
      "Dev loss: 2.0912\n"
     ]
    }
   ],
   "source": [
    "def compute_loss_torch(split):\n",
    "    model.inference_mode()\n",
    "    X, y = {\n",
    "        'train': (X_train, y_train),\n",
    "        'dev': (X_dev, y_dev),\n",
    "        'test': (X_test, y_dev)\n",
    "    }[split]\n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    model.training_mode()\n",
    "    return loss\n",
    "\n",
    "print(f'Training loss: {compute_loss_torch('train'):.4f}')\n",
    "print(f'Dev loss: {compute_loss_torch('dev'):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f7c7467-15ad-4990-b3a0-8553851025a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "carlinio.\n",
      "xier.\n",
      "alianna.\n",
      "thyla.\n",
      "cari.\n",
      "arif.\n",
      "kylee.\n",
      "revvanis.\n",
      "kelleente.\n"
     ]
    }
   ],
   "source": [
    "def make_name_torch(k):\n",
    "    model.inference_mode()\n",
    "    g = torch.Generator().manual_seed(2147483647 + k)\n",
    "    out = []\n",
    "    context = [0] * context_len\n",
    "    while True:\n",
    "        logits = model(torch.tensor([context]))\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        i = torch.multinomial(p, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [i]\n",
    "        out.append(i)\n",
    "        if i == 0:\n",
    "            break\n",
    "    model.training_mode()\n",
    "    return ''.join([itos[i] for i in out])\n",
    "\n",
    "for k in range(10):\n",
    "    print(make_name_torch(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d3d09-ad81-440e-aabb-c1fab91ee306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
